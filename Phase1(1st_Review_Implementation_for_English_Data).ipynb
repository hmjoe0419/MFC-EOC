{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a51c45-053a-4ac1-944e-12bcef52cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Download stop words (only needs to be done once)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return '' \n",
    "    text = text.strip()\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from {file_path}...\")\n",
    "        data = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Dataset loaded successfully with {len(data)} rows.\")\n",
    "        \n",
    "        data['content_translated'] = data['content_translated'].fillna('')\n",
    "        data['labels'] = data['labels'].fillna('unknown')\n",
    "\n",
    "        data['text'] = data['content_translated'].apply(preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "        return data['text'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute TF-IDF embeddings\n",
    "def compute_tfidf_embeddings(texts, n_features=512):\n",
    "    logger.info(\"Computing TF-IDF embeddings...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(texts).toarray()\n",
    "    rbf_sampler = RBFSampler(gamma=1.0, n_components=n_features, random_state=42)\n",
    "    reduced_tfidf_embeddings = rbf_sampler.fit_transform(tfidf_embeddings)\n",
    "    logger.info(\"TF-IDF embeddings computed successfully.\")\n",
    "    return reduced_tfidf_embeddings\n",
    "\n",
    "# Function to compute BERT embeddings\n",
    "def compute_bert_embeddings(texts, batch_size=32):\n",
    "    logger.info(\"Computing BERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    \n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    logger.info(\"BERT embeddings computed successfully.\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Function to fuse TF-IDF and BERT embeddings\n",
    "def fuse_embeddings(tfidf_embeddings, bert_embeddings):\n",
    "    logger.info(\"Fusing TF-IDF and BERT embeddings...\")\n",
    "    fused = np.hstack((tfidf_embeddings, bert_embeddings))\n",
    "    logger.info(\"Embeddings fused successfully.\")\n",
    "    return fused\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(file_path):\n",
    "    try:\n",
    "        texts, labels = load_data(file_path)\n",
    "        logger.info(\"Encoding labels...\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "        tfidf_embeddings = compute_tfidf_embeddings(texts)\n",
    "        bert_embeddings = compute_bert_embeddings(texts.tolist())\n",
    "        fused_embeddings = fuse_embeddings(tfidf_embeddings, bert_embeddings)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(fused_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        predictions = classifier.predict(X_test)\n",
    "        report = classification_report(y_test, predictions, target_names=label_encoder.classes_, digits=2, output_dict=True)\n",
    "        \n",
    "        print(\"{:<20} {:<10} {:<10} {:<10} {:<10}\".format(\"Category\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"))\n",
    "        for category, metrics in report.items():\n",
    "            if category not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\n",
    "                    category, metrics['precision'], metrics['recall'], metrics['f1-score'], int(metrics['support'])\n",
    "                ))\n",
    "        print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\"Macro avg\", \n",
    "            report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score'], int(report['macro avg']['support'])))\n",
    "        print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\"Weighted avg\", \n",
    "            report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score'], int(report['weighted avg']['support'])))\n",
    "        print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "        print(f\"Macro Precision: {report['macro avg']['precision']:.4f}\")\n",
    "        print(f\"Macro Recall: {report['macro avg']['recall']:.4f}\")\n",
    "        print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File path to dataset\n",
    "file_path = \"Tamil_sentiment_analysis_translated.xlsx\"\n",
    "train_and_evaluate(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221dc19-3f6c-459c-9b29-088a719ec44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Download stop words (only needs to be done once)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return '' \n",
    "    text = text.strip()\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from {file_path}...\")\n",
    "        data = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Dataset loaded successfully with {len(data)} rows.\")\n",
    "        \n",
    "        data['content_translated'] = data['content_translated'].fillna('')\n",
    "        data['labels'] = data['labels'].fillna('unknown')\n",
    "\n",
    "        data['text'] = data['content_translated'].apply(preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "        return data['text'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute TF-IDF embeddings\n",
    "def compute_tfidf_embeddings(texts, n_features=512):\n",
    "    logger.info(\"Computing TF-IDF embeddings...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(texts).toarray()\n",
    "    rbf_sampler = RBFSampler(gamma=1.0, n_components=n_features, random_state=42)\n",
    "    reduced_tfidf_embeddings = rbf_sampler.fit_transform(tfidf_embeddings)\n",
    "    logger.info(\"TF-IDF embeddings computed successfully.\")\n",
    "    return reduced_tfidf_embeddings\n",
    "\n",
    "# Function to compute BERT embeddings\n",
    "def compute_bert_embeddings(texts, batch_size=32):\n",
    "    logger.info(\"Computing BERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    \n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    logger.info(\"BERT embeddings computed successfully.\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Function to fuse TF-IDF and BERT embeddings\n",
    "def fuse_embeddings(tfidf_embeddings, bert_embeddings):\n",
    "    logger.info(\"Fusing TF-IDF and BERT embeddings...\")\n",
    "    fused = np.hstack((tfidf_embeddings, bert_embeddings))\n",
    "    logger.info(\"Embeddings fused successfully.\")\n",
    "    return fused\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(file_path):\n",
    "    try:\n",
    "        texts, labels = load_data(file_path)\n",
    "        logger.info(\"Encoding labels...\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "        tfidf_embeddings = compute_tfidf_embeddings(texts)\n",
    "        bert_embeddings = compute_bert_embeddings(texts.tolist())\n",
    "        fused_embeddings = fuse_embeddings(tfidf_embeddings, bert_embeddings)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(fused_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        predictions = classifier.predict(X_test)\n",
    "        report = classification_report(y_test, predictions, target_names=label_encoder.classes_, digits=2, output_dict=True)\n",
    "        \n",
    "        print(\"{:<20} {:<10} {:<10} {:<10} {:<10}\".format(\"Category\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"))\n",
    "        for category, metrics in report.items():\n",
    "            if category not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\n",
    "                    category, metrics['precision'], metrics['recall'], metrics['f1-score'], int(metrics['support'])\n",
    "                ))\n",
    "        print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\"Macro avg\", \n",
    "            report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score'], int(report['macro avg']['support'])))\n",
    "        print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\"Weighted avg\", \n",
    "            report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score'], int(report['weighted avg']['support'])))\n",
    "        print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "        print(f\"Macro Precision: {report['macro avg']['precision']:.4f}\")\n",
    "        print(f\"Macro Recall: {report['macro avg']['recall']:.4f}\")\n",
    "        print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File path to dataset\n",
    "file_path = \"Tamil_sentiment_analysis_translated.xlsx\"\n",
    "train_and_evaluate(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd44102-1b6e-4c26-8437-d12b7e34df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG-Boost Classifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Download stop words (only needs to be done once)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return '' \n",
    "    text = text.strip()\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from {file_path}...\")\n",
    "        data = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Dataset loaded successfully with {len(data)} rows.\")\n",
    "        \n",
    "        data['content_translated'] = data['content_translated'].fillna('')\n",
    "        data['labels'] = data['labels'].fillna('unknown')\n",
    "\n",
    "        data['text'] = data['content_translated'].apply(preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "        return data['text'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute TF-IDF embeddings\n",
    "def compute_tfidf_embeddings(texts, n_features=512):\n",
    "    logger.info(\"Computing TF-IDF embeddings...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(texts).toarray()\n",
    "    rbf_sampler = RBFSampler(gamma=1.0, n_components=n_features, random_state=42)\n",
    "    reduced_tfidf_embeddings = rbf_sampler.fit_transform(tfidf_embeddings)\n",
    "    logger.info(\"TF-IDF embeddings computed successfully.\")\n",
    "    return reduced_tfidf_embeddings\n",
    "\n",
    "# Function to compute BERT embeddings\n",
    "def compute_bert_embeddings(texts, batch_size=32):\n",
    "    logger.info(\"Computing BERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    \n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    logger.info(\"BERT embeddings computed successfully.\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Function to fuse TF-IDF and BERT embeddings\n",
    "def fuse_embeddings(tfidf_embeddings, bert_embeddings):\n",
    "    logger.info(\"Fusing TF-IDF and BERT embeddings...\")\n",
    "    fused = np.hstack((tfidf_embeddings, bert_embeddings))\n",
    "    logger.info(\"Embeddings fused successfully.\")\n",
    "    return fused\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(file_path):\n",
    "    try:\n",
    "        texts, labels = load_data(file_path)\n",
    "        logger.info(\"Encoding labels...\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "        tfidf_embeddings = compute_tfidf_embeddings(texts)\n",
    "        bert_embeddings = compute_bert_embeddings(texts.tolist())\n",
    "        fused_embeddings = fuse_embeddings(tfidf_embeddings, bert_embeddings)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(fused_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        predictions = classifier.predict(X_test)\n",
    "        report = classification_report(y_test, predictions, target_names=label_encoder.classes_, digits=2, output_dict=True)\n",
    "        \n",
    "        print(\"{:<20} {:<10} {:<10} {:<10} {:<10}\".format(\"Category\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"))\n",
    "        for category, metrics in report.items():\n",
    "            if category not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\n",
    "                    category, metrics['precision'], metrics['recall'], metrics['f1-score'], int(metrics['support'])\n",
    "                ))\n",
    "        print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\"Macro avg\", \n",
    "            report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score'], int(report['macro avg']['support'])))\n",
    "        print(\"{:<20} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\".format(\"Weighted avg\", \n",
    "            report['weighted avg']['precision'], report['weighted avg']['recall'], report['weighted avg']['f1-score'], int(report['weighted avg']['support'])))\n",
    "        print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "        print(f\"Macro Precision: {report['macro avg']['precision']:.4f}\")\n",
    "        print(f\"Macro Recall: {report['macro avg']['recall']:.4f}\")\n",
    "        print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File path to dataset\n",
    "file_path = \"Tamil_sentiment_analysis_translated.xlsx\"\n",
    "train_and_evaluate(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
