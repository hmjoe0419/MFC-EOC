{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12a61e-267e-4211-9ab9-68914d986ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 22:08:23,885 - INFO - Loading dataset from C:\\\\Users\\\\haris\\\\OneDrive\\\\Desktop\\\\Sentiment Analysis\\\\PS_train(3).xlsx...\n",
      "2025-03-09 22:08:24,186 - INFO - Dataset loaded successfully with 4352 rows.\n",
      "2025-03-09 22:08:24,233 - INFO - Text preprocessing completed.\n",
      "2025-03-09 22:08:24,233 - INFO - Class distribution before balancing: Counter({3: 1361, 5: 790, 1: 637, 4: 575, 6: 412, 0: 406, 2: 171})\n",
      "2025-03-09 22:08:24,233 - INFO - Computing mBERT embeddings...\n",
      "2025-03-09 22:08:27,148 - INFO - Processed 0/4352 texts...\n",
      "2025-03-09 22:09:10,736 - INFO - Processed 320/4352 texts...\n",
      "2025-03-09 22:09:34,567 - INFO - Processed 640/4352 texts...\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST CLASSIFIER\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import re\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to preprocess Tamil text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return '' \n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    return text\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from {file_path}...\")\n",
    "        data = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Dataset loaded successfully with {len(data)} rows.\")\n",
    "        \n",
    "        data['content'] = data['content'].fillna('')\n",
    "        data['labels'] = data['labels'].fillna('unknown')\n",
    "        \n",
    "        data['text'] = data['content'].apply(preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "        return data['text'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute mBERT embeddings\n",
    "def compute_mbert_embeddings(texts, batch_size=32):\n",
    "    logger.info(\"Computing mBERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Using CLS token\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    logger.info(\"mBERT embeddings computed successfully.\")\n",
    "    return embeddings\n",
    "\n",
    "# Function to balance classes using SMOTE + Tomek Links\n",
    "def balance_classes(X, y):\n",
    "    logger.info(\"Applying SMOTE + Tomek Links to balance classes...\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "    logger.info(f\"Class distribution after balancing: {Counter(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(file_path):\n",
    "    try:\n",
    "        texts, labels = load_data(file_path)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        logger.info(f\"Class distribution before balancing: {Counter(labels_encoded)}\")\n",
    "        \n",
    "        mbert_embeddings = compute_mbert_embeddings(texts.tolist())\n",
    "        X_resampled, y_resampled = balance_classes(mbert_embeddings, labels_encoded)\n",
    "        \n",
    "        # Train-test split (80% training, 20% testing)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "        \n",
    "        logger.info(\"Training the model...\")\n",
    "        classifier = RandomForestClassifier(random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = classifier.predict(X_test)\n",
    "        \n",
    "        logger.info(\"Classification Report:\")\n",
    "        print(classification_report(y_test, predictions, target_names=[label_mapping[i] for i in range(len(label_mapping))]))\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "        macro_precision = precision_score(y_test, predictions, average='macro')\n",
    "        macro_recall = recall_score(y_test, predictions, average='macro')\n",
    "        \n",
    "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        logger.info(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        logger.info(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File path to dataset\n",
    "file_path = r\"C:\\\\Users\\\\haris\\\\OneDrive\\\\Desktop\\\\Sentiment Analysis\\\\PS_train(3).xlsx\"\n",
    "train_and_evaluate(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513172f-17ca-4781-acc6-e4d2431bc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM CLASSIFIER\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import re\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to preprocess Tamil text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return '' \n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    return text\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from {file_path}...\")\n",
    "        data = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Dataset loaded successfully with {len(data)} rows.\")\n",
    "        \n",
    "        data['content'] = data['content'].fillna('')\n",
    "        data['labels'] = data['labels'].fillna('unknown')\n",
    "        \n",
    "        data['text'] = data['content'].apply(preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "        return data['text'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute mBERT embeddings\n",
    "def compute_mbert_embeddings(texts, batch_size=32):\n",
    "    logger.info(\"Computing mBERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Using CLS token\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    logger.info(\"mBERT embeddings computed successfully.\")\n",
    "    return embeddings\n",
    "\n",
    "# Function to balance classes using SMOTE + Tomek Links\n",
    "def balance_classes(X, y):\n",
    "    logger.info(\"Applying SMOTE + Tomek Links to balance classes...\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "    logger.info(f\"Class distribution after balancing: {Counter(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(file_path):\n",
    "    try:\n",
    "        texts, labels = load_data(file_path)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        logger.info(f\"Class distribution before balancing: {Counter(labels_encoded)}\")\n",
    "        \n",
    "        mbert_embeddings = compute_mbert_embeddings(texts.tolist())\n",
    "        X_resampled, y_resampled = balance_classes(mbert_embeddings, labels_encoded)\n",
    "        \n",
    "        # Train-test split (80% training, 20% testing)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "        \n",
    "        logger.info(\"Training the model...\")\n",
    "        classifier = SVC(kernel='linear', random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = classifier.predict(X_test)\n",
    "        \n",
    "        logger.info(\"Classification Report:\")\n",
    "        print(classification_report(y_test, predictions, target_names=[label_mapping[i] for i in range(len(label_mapping))]))\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "        macro_precision = precision_score(y_test, predictions, average='macro')\n",
    "        macro_recall = recall_score(y_test, predictions, average='macro')\n",
    "        \n",
    "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        logger.info(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        logger.info(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File path to dataset\n",
    "file_path = r\"C:\\\\Users\\\\haris\\\\OneDrive\\\\Desktop\\\\Sentiment Analysis\\\\PS_train(3).xlsx\"\n",
    "train_and_evaluate(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff7801-744a-453e-869b-3878e799d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG-Boost Classifier\n",
    "\n",
    "import pandas as pd  # Data manipulation\n",
    "import numpy as np  # Numerical operations\n",
    "import matplotlib.pyplot as plt  # Visualizations\n",
    "from sklearn.model_selection import train_test_split  # Train/test split\n",
    "from transformers import BertTokenizer, BertModel  # BERT models\n",
    "from xgboost import XGBClassifier  # XGBoost classifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score  # Evaluation metrics\n",
    "import torch  # PyTorch\n",
    "import re  # Regular expressions\n",
    "import logging  # Logging\n",
    "from sklearn.preprocessing import LabelEncoder  # Label encoding\n",
    "from imblearn.combine import SMOTETomek  # SMOTE-Tomek balancing\n",
    "from collections import Counter  # Counting elements\n",
    "\n",
    "# Configure logging for better debugging and monitoring\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to preprocess Tamil text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input Tamil text by removing extra spaces, URLs, and mentions.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return ''  # Handling non-string input\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    return text\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from an Excel file, preprocesses the text, and handles missing values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading dataset from {file_path}...\")\n",
    "        data = pd.read_excel(file_path, engine=\"openpyxl\")  # Load dataset\n",
    "        logger.info(f\"Dataset loaded successfully with {len(data)} rows.\")\n",
    "        \n",
    "        # Handling missing values in content and labels\n",
    "        data['content'] = data['content'].fillna('')\n",
    "        data['labels'] = data['labels'].fillna('unknown')\n",
    "        \n",
    "        # Apply text preprocessing function to each row\n",
    "        data['text'] = data['content'].apply(preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "        return data['text'], data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute mBERT embeddings\n",
    "def compute_mbert_embeddings(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Converts Tamil text into numerical embeddings using mBERT (multilingual BERT).\n",
    "    \"\"\"\n",
    "    logger.info(\"Computing mBERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')  # Load tokenizer\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')  # Load mBERT model\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]  # Process batch of texts\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)  # Get model outputs\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Extract CLS token representation\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    logger.info(\"mBERT embeddings computed successfully.\")\n",
    "    return embeddings\n",
    "\n",
    "# Function to plot class distribution\n",
    "def plot_class_distribution(labels, title):\n",
    "    \"\"\"\n",
    "    Plots the distribution of class labels to analyze class imbalance.\n",
    "    \"\"\"\n",
    "    label_counts = Counter(labels)  # Count occurrences of each label\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(label_counts.keys(), label_counts.values(), color='skyblue')  # Create bar plot\n",
    "    plt.xlabel('Class Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=0)  # Keep x-axis labels straight\n",
    "    plt.show()\n",
    "\n",
    "# Function to balance classes using SMOTE + Tomek Links\n",
    "def balance_classes(X, y):\n",
    "    \"\"\"\n",
    "    Balances the dataset using SMOTE + Tomek Links to handle class imbalance issues.\n",
    "    \"\"\"\n",
    "    logger.info(\"Applying SMOTE + Tomek Links to balance classes...\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)  # Apply resampling\n",
    "    logger.info(f\"Class distribution after balancing: {Counter(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(file_path):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost classifier using mBERT embeddings and balanced data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        texts, labels = load_data(file_path)\n",
    "        \n",
    "        # Encode categorical labels into numerical format\n",
    "        label_encoder = LabelEncoder()\n",
    "        labels_encoded = label_encoder.fit_transform(labels)\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        \n",
    "        logger.info(f\"Class distribution before balancing: {Counter(labels_encoded)}\")\n",
    "        plot_class_distribution(labels_encoded, \"Class Distribution Before Balancing\")\n",
    "        \n",
    "        # Compute embeddings from text\n",
    "        mbert_embeddings = compute_mbert_embeddings(texts.tolist())\n",
    "        \n",
    "        # Balance dataset using SMOTE + Tomek Links\n",
    "        X_resampled, y_resampled = balance_classes(mbert_embeddings, labels_encoded)\n",
    "        plot_class_distribution(y_resampled, \"Class Distribution After Balancing\")\n",
    "        \n",
    "        # Train-test split (80% training, 20% testing)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "        \n",
    "        logger.info(\"Training the model...\")\n",
    "        classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        classifier.fit(X_train, y_train)  # Train classifier\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        predictions = classifier.predict(X_test)\n",
    "        \n",
    "        # Print classification report with metrics\n",
    "        logger.info(\"Classification Report:\")\n",
    "        print(classification_report(y_test, predictions, target_names=[label_mapping[i] for i in range(len(label_mapping))]))\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "        macro_precision = precision_score(y_test, predictions, average='macro')\n",
    "        macro_recall = recall_score(y_test, predictions, average='macro')\n",
    "        \n",
    "        # Log and print evaluation results\n",
    "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        logger.info(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        logger.info(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File path to dataset\n",
    "file_path = r\"C:\\\\Users\\\\haris\\\\OneDrive\\\\Desktop\\\\Sentiment Analysis\\\\PS_train(3).xlsx\"\n",
    "\n",
    "# Execute training and evaluation\n",
    "train_and_evaluate(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
