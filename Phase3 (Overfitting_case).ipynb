{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb383c2-bf5f-49d1-8a27-9175539fafb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 22:33:08,818 - INFO - Loading training dataset from C:\\Users\\haris\\OneDrive\\Desktop\\Sentiment Analysis\\PS_train(3).xlsx...\n",
      "2025-03-09 22:33:08,994 - INFO - Training dataset loaded successfully with 4352 rows.\n",
      "2025-03-09 22:33:08,994 - INFO - Loading testing dataset from C:\\Users\\haris\\OneDrive\\Desktop\\Sentiment Analysis\\PS_test.xlsx...\n",
      "2025-03-09 22:33:09,042 - INFO - Testing dataset loaded successfully with 544 rows.\n",
      "2025-03-09 22:33:09,074 - INFO - Text preprocessing completed.\n",
      "2025-03-09 22:33:09,074 - INFO - Class distribution before balancing: Counter({3: 1361, 5: 790, 1: 637, 4: 575, 6: 412, 0: 406, 2: 171})\n",
      "2025-03-09 22:33:09,074 - INFO - Computing mBERT embeddings...\n",
      "2025-03-09 22:33:16,323 - INFO - Processed 0/4352 texts...\n",
      "2025-03-09 22:34:05,840 - INFO - Processed 320/4352 texts...\n",
      "2025-03-09 22:34:31,060 - INFO - Processed 640/4352 texts...\n",
      "2025-03-09 22:35:06,710 - INFO - Processed 960/4352 texts...\n",
      "2025-03-09 22:35:34,050 - INFO - Processed 1280/4352 texts...\n",
      "2025-03-09 22:36:09,658 - INFO - Processed 1600/4352 texts...\n",
      "2025-03-09 22:36:36,131 - INFO - Processed 1920/4352 texts...\n",
      "2025-03-09 22:37:08,158 - INFO - Processed 2240/4352 texts...\n",
      "2025-03-09 22:37:45,031 - INFO - Processed 2560/4352 texts...\n",
      "2025-03-09 22:38:19,918 - INFO - Processed 2880/4352 texts...\n",
      "2025-03-09 22:38:51,337 - INFO - Processed 3200/4352 texts...\n",
      "2025-03-09 22:39:30,566 - INFO - Processed 3520/4352 texts...\n",
      "2025-03-09 22:40:06,624 - INFO - Processed 3840/4352 texts...\n",
      "2025-03-09 22:40:40,051 - INFO - Processed 4160/4352 texts...\n",
      "2025-03-09 22:40:55,667 - INFO - mBERT embeddings computed successfully.\n",
      "2025-03-09 22:40:55,757 - INFO - Applying SMOTE + Tomek Links to balance classes...\n",
      "C:\\Users\\haris\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\haris\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haris\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haris\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\haris\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-03-09 22:40:57,448 - INFO - Class distribution after balancing: Counter({2: 1361, 0: 1355, 6: 1350, 4: 1337, 5: 1336, 1: 1332, 3: 1310})\n",
      "2025-03-09 22:40:57,449 - INFO - Computing mBERT embeddings...\n",
      "2025-03-09 22:41:02,961 - INFO - Processed 0/544 texts...\n",
      "2025-03-09 22:41:38,464 - INFO - Processed 320/544 texts...\n",
      "2025-03-09 22:42:18,013 - INFO - mBERT embeddings computed successfully.\n",
      "2025-03-09 22:42:18,086 - INFO - Training the model...\n",
      "C:\\Users\\haris\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:42:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2025-03-09 22:42:50,231 - INFO - Classification Report:\n",
      "2025-03-09 22:42:50,253 - INFO - Accuracy: 0.2904\n",
      "2025-03-09 22:42:50,254 - INFO - Macro Precision: 0.3160\n",
      "2025-03-09 22:42:50,255 - INFO - Macro Recall: 0.2985\n",
      "2025-03-09 22:42:50,255 - INFO - Macro F1-score: 0.3055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Negative       0.13      0.13      0.13        46\n",
      "          Neutral       0.16      0.13      0.14        70\n",
      "None of the above       0.95      0.84      0.89        25\n",
      "      Opinionated       0.36      0.45      0.40       171\n",
      "         Positive       0.20      0.19      0.19        75\n",
      "        Sarcastic       0.27      0.24      0.25       106\n",
      "    Substantiated       0.13      0.12      0.12        51\n",
      "\n",
      "         accuracy                           0.29       544\n",
      "        macro avg       0.32      0.30      0.31       544\n",
      "     weighted avg       0.28      0.29      0.28       544\n",
      "\n",
      "Accuracy: 0.2904\n",
      "Macro Precision: 0.3160\n",
      "Macro Recall: 0.2985\n",
      "Macro F1-score: 0.3055\n"
     ]
    }
   ],
   "source": [
    "# XG-Boost Classifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import re\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to preprocess Tamil text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(\"Non-string input detected; replacing with empty string.\")\n",
    "        return '' \n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    return text\n",
    "\n",
    "# Function to load and preprocess datasets\n",
    "def load_data(train_path, test_path):\n",
    "    try:\n",
    "        logger.info(f\"Loading training dataset from {train_path}...\")\n",
    "        train_data = pd.read_excel(train_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Training dataset loaded successfully with {len(train_data)} rows.\")\n",
    "        \n",
    "        logger.info(f\"Loading testing dataset from {test_path}...\")\n",
    "        test_data = pd.read_excel(test_path, engine=\"openpyxl\")\n",
    "        logger.info(f\"Testing dataset loaded successfully with {len(test_data)} rows.\")\n",
    "        \n",
    "        train_data['content'] = train_data['content'].fillna('')\n",
    "        train_data['labels'] = train_data['labels'].fillna('unknown')\n",
    "        test_data['content'] = test_data['content'].fillna('')\n",
    "        test_data['labels'] = test_data['labels'].fillna('unknown')\n",
    "        \n",
    "        train_data['text'] = train_data['content'].apply(preprocess_text)\n",
    "        test_data['text'] = test_data['content'].apply(preprocess_text)\n",
    "        \n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "        return train_data['text'], train_data['labels'], test_data['text'], test_data['labels']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading datasets: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to compute mBERT embeddings\n",
    "def compute_mbert_embeddings(texts, batch_size=32):\n",
    "    logger.info(\"Computing mBERT embeddings...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Using CLS token\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                logger.info(f\"Processed {i}/{len(texts)} texts...\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    logger.info(\"mBERT embeddings computed successfully.\")\n",
    "    return embeddings\n",
    "\n",
    "# Function to balance classes using SMOTE + Tomek Links\n",
    "def balance_classes(X, y):\n",
    "    logger.info(\"Applying SMOTE + Tomek Links to balance classes...\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "    logger.info(f\"Class distribution after balancing: {Counter(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(train_path, test_path):\n",
    "    try:\n",
    "        train_texts, train_labels, test_texts, test_labels = load_data(train_path, test_path)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "        test_labels_encoded = label_encoder.transform(test_labels)\n",
    "        label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "        \n",
    "        logger.info(f\"Class distribution before balancing: {Counter(train_labels_encoded)}\")\n",
    "        \n",
    "        train_embeddings = compute_mbert_embeddings(train_texts.tolist())\n",
    "        X_resampled, y_resampled = balance_classes(train_embeddings, train_labels_encoded)\n",
    "        \n",
    "        test_embeddings = compute_mbert_embeddings(test_texts.tolist())\n",
    "        \n",
    "        logger.info(\"Training the model...\")\n",
    "        classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        classifier.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        predictions = classifier.predict(test_embeddings)\n",
    "        \n",
    "        logger.info(\"Classification Report:\")\n",
    "        print(classification_report(test_labels_encoded, predictions, target_names=[label_mapping[i] for i in range(len(label_mapping))]))\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels_encoded, predictions)\n",
    "        macro_f1 = f1_score(test_labels_encoded, predictions, average='macro')\n",
    "        macro_precision = precision_score(test_labels_encoded, predictions, average='macro')\n",
    "        macro_recall = recall_score(test_labels_encoded, predictions, average='macro')\n",
    "        \n",
    "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        logger.info(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        logger.info(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "        print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "        print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training and evaluation: {e}\")\n",
    "        raise\n",
    "\n",
    "# File paths to datasets\n",
    "train_file_path = r\"C:\\Users\\haris\\OneDrive\\Desktop\\Sentiment Analysis\\PS_train(3).xlsx\"\n",
    "test_file_path =  r\"C:\\Users\\haris\\OneDrive\\Desktop\\Sentiment Analysis\\PS_test.xlsx\"\n",
    "train_and_evaluate(train_file_path, test_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751c41f-5500-4061-a805-3a2a6f7a873a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
