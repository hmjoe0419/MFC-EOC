import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.kernel_approximation import RBFSampler
from transformers import BertTokenizer, BertModel
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import torch
import re
import nltk
from nltk.corpus import stopwords
import emoji
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger()

# Download stop words (only needs to be done once)
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Function to preprocess text
def preprocess_text(text):
    if not isinstance(text, str):  # Check if the input is not a string
        logger.warning("Non-string input detected; replacing with empty string.")
        return '' 
    text = text.strip()  # Remove extra spaces
    text = emoji.replace_emoji(text, replace='')  # Remove emojis
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    words = text.split()  # Tokenize text
    words = [word for word in words if word.lower() not in stop_words]  # Remove stop words
    return ' '.join(words)

# Function to load and preprocess the dataset
def load_data(file_path):
    try:
        logger.info(f"Loading dataset from {file_path}...")
        data = pd.read_excel(file_path, engine="openpyxl")  # Ensure correct engine for Excel
        logger.info(f"Dataset loaded successfully with {len(data)} rows.")
        
        # Handle missing values in required columns
        data['content_translated'] = data['content_translated'].fillna('')
        data['labels'] = data['labels'].fillna('unknown')

        # Preprocess text column
        data['text'] = data['content_translated'].apply(preprocess_text)
        logger.info("Text preprocessing completed.")

        return data['text'], data['labels']
    except Exception as e:
        logger.error(f"Error loading dataset: {e}")
        raise

# Function to compute TF-IDF embeddings
def compute_tfidf_embeddings(texts, n_features=512):
    logger.info("Computing TF-IDF embeddings...")
    tfidf_vectorizer = TfidfVectorizer(max_features=5000)
    tfidf_embeddings = tfidf_vectorizer.fit_transform(texts).toarray()
    rbf_sampler = RBFSampler(gamma=1.0, n_components=n_features, random_state=42)
    reduced_tfidf_embeddings = rbf_sampler.fit_transform(tfidf_embeddings)
    logger.info("TF-IDF embeddings computed successfully.")
    return reduced_tfidf_embeddings

# Function to compute BERT embeddings in batches
def compute_bert_embeddings(texts, batch_size=32):
    logger.info("Computing BERT embeddings...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    embeddings = []
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=512)
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()
            embeddings.extend(batch_embeddings)

            if i % (batch_size * 10) == 0:
                logger.info(f"Processed {i}/{len(texts)} texts...")
    logger.info("BERT embeddings computed successfully.")
    return np.array(embeddings)

# Function to combine TF-IDF and BERT embeddings
def fuse_embeddings(tfidf_embeddings, bert_embeddings):
    logger.info("Fusing TF-IDF and BERT embeddings...")
    fused = np.hstack((tfidf_embeddings, bert_embeddings))
    logger.info("Embeddings fused successfully.")
    return fused

# Main function to train and evaluate the model
def train_and_evaluate(file_path):
    try:
        # Load and preprocess data
        texts, labels = load_data(file_path)

        # Convert labels to numerical format
        logger.info("Encoding labels...")
        labels = LabelEncoder().fit_transform(labels)

        # Compute embeddings
        tfidf_embeddings = compute_tfidf_embeddings(texts)
        bert_embeddings = compute_bert_embeddings(texts.tolist())
        fused_embeddings = fuse_embeddings(tfidf_embeddings, bert_embeddings)

        # K-Fold cross-validation
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        labels = np.array(labels)

        smote = SMOTE(random_state=42)

        for fold, (train_index, test_index) in enumerate(kf.split(fused_embeddings)):
            logger.info(f"\nStarting Fold {fold + 1}...")
            X_train, X_test = fused_embeddings[train_index], fused_embeddings[test_index]
            y_train, y_test = labels[train_index], labels[test_index]

            # Apply SMOTE to balance class distribution in training data
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
            logger.info("SMOTE resampling applied to the training data.")

            classifier = RandomForestClassifier(random_state=42)
            classifier.fit(X_train_resampled, y_train_resampled)

            predictions = classifier.predict(X_test)
            logger.info(f"Fold {fold + 1} Classification Report:")
            print(classification_report(y_test, predictions))
    except Exception as e:
        logger.error(f"Error during training and evaluation: {e}")
        raise

# File path to dataset
file_path = r"C:\Users\haris\OneDrive\Desktop\Tamil_sentiment_analysis_translated(1).xlsx" # Ensure this file exists in the working directory
train_and_evaluate(file_path)
